{"cells":[{"cell_type":"markdown","metadata":{"id":"884898d7"},"source":["##  singel model Advance perameters üöÄ\n","\n","## Introduction üåü\n","Welcome to this Jupyter notebook developed for the Optiver - Trading at the Close This notebook is designed to assist you in the competition, Predict US stocks closing movements\n","\n","\n","### Inspiration and Credits üôå\n","This notebook draws inspiration from the remarkable work of Angle, which can be found in [this Kaggle project](https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook). Special thanks to Angle for sharing valuable insights and code.\n","\n","üåü Dive into my profile and explore other public projects. Don't forget to share your feedback and experiences!\n","üëâ [Visit my Profile](https://www.kaggle.com/zulqarnainali) üëà\n","\n","üôè Your time and consideration are greatly appreciated. If you find this notebook valuable, please give it a thumbs-up! üëç\n","\n","## Purpose üéØ\n","This notebook serves several primary purposes:\n","- Load and preprocess the competition data üìÅ\n","- Engineer pertinent features for training predictive models üèãÔ∏è‚Äç‚ôÇÔ∏è\n","- Train models to predict the target variable üß†\n","- Submit predictions to the competition environment üì§\n","\n","## Notebook Structure üìö\n","This notebook follows a structured approach:\n","1. **Data Preparation**: We load and preprocess the competition data in this section.\n","2. **Feature Engineering**: The generation and selection of relevant features for model training are covered here.\n","3. **Model Training**: Machine learning models are trained on the prepared data.\n","4. **Prediction and Submission**: We make predictions on the test data and submit them for evaluation.\n","\n","## How to Use üõ†Ô∏è\n","To make the most of this notebook, please adhere to these steps:\n","1. Ensure you have the competition data and environment ready.\n","2. Execute the cells in order for data preparation, feature engineering, model training, and prediction submission.\n","3. Customize and adjust the code as needed to enhance model performance or experiment with different approaches.\n","\n","**Note**: Be sure to replace any placeholder paths or configurations with your specific information.\n","\n","## Acknowledgments üôè\n","We extend our gratitude to the Optiver organizers for providing the dataset and hosting the competition.\n","\n","Let's embark on this journey! Don't hesitate to reach out if you have questions or require assistance along the way.\n","üëâ [Visit my Profile](https://www.kaggle.com/zulqarnainali) üëà"],"id":"884898d7"},{"cell_type":"markdown","metadata":{"id":"75fa34f4"},"source":["## üßπ Importing necessary libraries"],"id":"75fa34f4"},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":496,"status":"ok","timestamp":1701534962925,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"bb8d2906"},"outputs":[],"source":["import gc  # Garbage collection for memory management\n","import os  # Operating system-related functions\n","import time  # Time-related functions\n","import warnings  # Handling warnings\n","from itertools import combinations  # For creating combinations of elements\n","from warnings import simplefilter  # Simplifying warning handling\n","\n","# üì¶ Importing machine learning libraries\n","import joblib  # For saving and loading models\n","import lightgbm as lgb  # LightGBM gradient boosting framework\n","import numpy as np  # Numerical operations\n","import pandas as pd  # Data manipulation and analysis\n","from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n","from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n","\n","# ü§ê Disable warnings to keep the code clean\n","warnings.filterwarnings(\"ignore\")\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","\n","# üìä Define flags and variables\n","is_offline = False  # Flag for online/offline mode\n","is_train = True  # Flag for training mode\n","is_infer = True  # Flag for inference mode\n","max_lookback = np.nan  # Maximum lookback (not specified)\n","split_day = 480  # Split day for time series data"],"id":"bb8d2906"},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1701534964672,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"Ouc4eoTUdHbO","outputId":"d6d0ef8f-1780-4aca-d50b-209f250f15fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"Ouc4eoTUdHbO"},{"cell_type":"markdown","metadata":{"id":"c939f7e9"},"source":["## üìä Data Loading and Preprocessing üìä\n","\n","\n","\n","\n"],"id":"c939f7e9"},{"cell_type":"markdown","metadata":{"id":"ac62feb7"},"source":["**Explaination**\n","\n","1. `df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")`\n","   - This line reads a CSV (Comma-Separated Values) file named \"train.csv\" using the Pandas library and assigns the resulting DataFrame to the variable `df`. The CSV file is expected to be located at the specified file path, \"/kaggle/input/optiver-trading-at-the-close/train.csv\". This line is loading a dataset from a file.\n","\n","2. `df = df.dropna(subset=[\"target\"])`\n","   - This line drops (removes) rows from the DataFrame `df` where there are missing values (NaN) in the \"target\" column. It uses the `dropna` method with the `subset` parameter set to \"target\" to specify that it should check for missing values in the \"target\" column and remove rows that have missing values. The updated DataFrame is assigned back to the variable `df`.\n","\n","3. `df.reset_index(drop=True, inplace=True)`\n","   - This line resets the index of the DataFrame `df`. When data is removed from a DataFrame, the index labels may have gaps or may not be sequential. This line resets the index to be sequential, starting from 0, and the old index is dropped. The `drop=True` parameter indicates that the old index should be dropped, and `inplace=True` means that this operation modifies the DataFrame in place.\n","\n","4. `df_shape = df.shape`\n","   - This line calculates the shape of the DataFrame `df`, which means it returns a tuple containing the number of rows and columns in the DataFrame. The result is assigned to the variable `df_shape`.\n","\n","To summarize, the code reads a dataset from a CSV file, removes rows with missing values in a specific column (\"target\"), resets the index of the DataFrame to make it sequential, and finally, it calculates and stores the shape of the resulting DataFrame in the `df_shape` variable."],"id":"ac62feb7"},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":10629,"status":"ok","timestamp":1701534975300,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"7e403d82"},"outputs":[],"source":["# üìÇ Read the dataset from a CSV file using Pandas\n","df = pd.read_csv(\"/content/drive/MyDrive/0-Fall 2023/15-072_Analytics-Edge/edge-project-2023/1-data/train.csv\")\n","\n","# üßπ Remove rows with missing values in the \"target\" column\n","df = df.dropna(subset=[\"target\"])\n","\n","# üîÅ Reset the index of the DataFrame and apply the changes in place\n","df.reset_index(drop=True, inplace=True)\n","\n","# üìè Get the shape of the DataFrame (number of rows and columns)\n","df_shape = df.shape\n"],"id":"7e403d82"},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1701534975300,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"G1rtJpiRc71u","outputId":"dd5e1683-2b8a-4b31-99aa-043e67ad4f3c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5237892, 17)"]},"metadata":{},"execution_count":54}],"source":["df.shape"],"id":"G1rtJpiRc71u"},{"cell_type":"code","source":["unique_date_ids = df['date_id'].nunique()\n","unique_date_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pmi0kWukWrUp","executionInfo":{"status":"ok","timestamp":1701534975300,"user_tz":300,"elapsed":13,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"}},"outputId":"a9975b28-1632-4b00-b096-dfb775bb261e"},"id":"Pmi0kWukWrUp","execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["481"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3307,"status":"ok","timestamp":1701534978596,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"48jzU-jSc71v","outputId":"63e8877c-be9e-46ed-c2bc-0c92780a7ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Head of DataFrame:\n","   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n","0         0        0                  0      3180602.69   \n","1         1        0                  0       166603.91   \n","2         2        0                  0       302879.87   \n","3         3        0                  0     11917682.27   \n","4         4        0                  0       447549.96   \n","\n","   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n","0                        1         0.999812   13380276.64        NaN   \n","1                       -1         0.999896    1642214.25        NaN   \n","2                       -1         0.999561    1819368.03        NaN   \n","3                       -1         1.000171   18389745.62        NaN   \n","4                       -1         0.999532   17860614.95        NaN   \n","\n","   near_price  bid_price  bid_size  ask_price   ask_size  wap    target  \\\n","0         NaN   0.999812  60651.50   1.000026    8493.03  1.0 -3.029704   \n","1         NaN   0.999896   3233.04   1.000660   20605.09  1.0 -5.519986   \n","2         NaN   0.999403  37956.00   1.000298   18995.00  1.0 -8.389950   \n","3         NaN   0.999999   2324.90   1.000214  479032.40  1.0 -4.010200   \n","4         NaN   0.999394  16485.54   1.000016     434.10  1.0 -7.349849   \n","\n","   time_id row_id  \n","0        0  0_0_0  \n","1        0  0_0_1  \n","2        0  0_0_2  \n","3        0  0_0_3  \n","4        0  0_0_4  \n","\n","Data Types:\n","stock_id                     int64\n","date_id                      int64\n","seconds_in_bucket            int64\n","imbalance_size             float64\n","imbalance_buy_sell_flag      int64\n","reference_price            float64\n","matched_size               float64\n","far_price                  float64\n","near_price                 float64\n","bid_price                  float64\n","bid_size                   float64\n","ask_price                  float64\n","ask_size                   float64\n","wap                        float64\n","target                     float64\n","time_id                      int64\n","row_id                      object\n","dtype: object\n","\n","DataFrame Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5237892 entries, 0 to 5237891\n","Data columns (total 17 columns):\n"," #   Column                   Dtype  \n","---  ------                   -----  \n"," 0   stock_id                 int64  \n"," 1   date_id                  int64  \n"," 2   seconds_in_bucket        int64  \n"," 3   imbalance_size           float64\n"," 4   imbalance_buy_sell_flag  int64  \n"," 5   reference_price          float64\n"," 6   matched_size             float64\n"," 7   far_price                float64\n"," 8   near_price               float64\n"," 9   bid_price                float64\n"," 10  bid_size                 float64\n"," 11  ask_price                float64\n"," 12  ask_size                 float64\n"," 13  wap                      float64\n"," 14  target                   float64\n"," 15  time_id                  int64  \n"," 16  row_id                   object \n","dtypes: float64(11), int64(5), object(1)\n","memory usage: 679.4+ MB\n","\n","Describe:\n","           stock_id       date_id  seconds_in_bucket  imbalance_size  \\\n","count  5.237892e+06  5.237892e+06       5.237892e+06    5.237760e+06   \n","mean   9.928777e+01  2.415097e+02       2.700008e+02    5.715293e+06   \n","std    5.787187e+01  1.385313e+02       1.587450e+02    2.051591e+07   \n","min    0.000000e+00  0.000000e+00       0.000000e+00    0.000000e+00   \n","25%    4.900000e+01  1.220000e+02       1.300000e+02    8.453415e+04   \n","50%    9.900000e+01  2.420000e+02       2.700000e+02    1.113604e+06   \n","75%    1.490000e+02  3.610000e+02       4.100000e+02    4.190951e+06   \n","max    1.990000e+02  4.800000e+02       5.400000e+02    2.982028e+09   \n","\n","       imbalance_buy_sell_flag  reference_price  matched_size     far_price  \\\n","count             5.237892e+06     5.237760e+06  5.237760e+06  2.343638e+06   \n","mean             -1.189620e-02     9.999955e-01  4.510025e+07  1.001713e+00   \n","std               8.853447e-01     2.532497e-03  1.398413e+08  7.214705e-01   \n","min              -1.000000e+00     9.352850e-01  4.316610e+03  7.700000e-05   \n","25%              -1.000000e+00     9.987630e-01  5.279575e+06  9.963320e-01   \n","50%               0.000000e+00     9.999670e-01  1.288264e+07  9.998830e-01   \n","75%               1.000000e+00     1.001174e+00  3.270013e+07  1.003318e+00   \n","max               1.000000e+00     1.077488e+00  7.713682e+09  4.379531e+02   \n","\n","         near_price     bid_price      bid_size     ask_price      ask_size  \\\n","count  2.380800e+06  5.237760e+06  5.237892e+06  5.237760e+06  5.237892e+06   \n","mean   9.996601e-01  9.997263e-01  5.181445e+04  1.000264e+00  5.357658e+04   \n","std    1.216920e-02  2.499345e-03  1.114221e+05  2.510042e-03  1.293563e+05   \n","min    7.869880e-01  9.349150e-01  5.600000e-01  9.398270e-01  5.900000e-01   \n","25%    9.971000e-01  9.985290e-01  7.375160e+03  9.990290e-01  7.824000e+03   \n","50%    9.998890e-01  9.997280e-01  2.196980e+04  1.000207e+00  2.301867e+04   \n","75%    1.002590e+00  1.000905e+00  5.583240e+04  1.001414e+00  5.787932e+04   \n","max    1.309732e+00  1.077488e+00  3.028784e+07  1.077836e+00  5.440500e+07   \n","\n","                wap        target       time_id  \n","count  5.237760e+06  5.237892e+06  5.237892e+06  \n","mean   9.999920e-01 -4.756125e-02  1.331003e+04  \n","std    2.497509e-03  9.452860e+00  7.619238e+03  \n","min    9.380080e-01 -3.852898e+02  0.000000e+00  \n","25%    9.987810e-01 -4.559755e+00  6.729000e+03  \n","50%    9.999970e-01 -6.020069e-02  1.334500e+04  \n","75%    1.001149e+00  4.409552e+00  1.990700e+04  \n","max    1.077675e+00  4.460704e+02  2.645400e+04  \n","\n","Shape of DataFrame:\n","(5237892, 17)\n"]}],"source":["# Display the first few rows of the DataFrame\n","print(\"Head of DataFrame:\")\n","print(df.head())\n","\n","# Display the data types of each column\n","print(\"\\nData Types:\")\n","print(df.dtypes)\n","\n","# Get a concise summary of the DataFrame\n","print(\"\\nDataFrame Info:\")\n","df.info()\n","\n","# Display statistics for numerical columns\n","print(\"\\nDescribe:\")\n","print(df.describe())\n","\n","# Display the shape of the DataFrame\n","print(\"\\nShape of DataFrame:\")\n","print(df.shape)"],"id":"48jzU-jSc71v"},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1701534978596,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"CZUR4yj2c71w","outputId":"aad149f2-779a-482d-aac6-645ab3ce9854"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n","5237887       195      480                540      2440722.89   \n","5237888       196      480                540       349510.47   \n","5237889       197      480                540            0.00   \n","5237890       198      480                540      1000898.84   \n","5237891       199      480                540      1884285.71   \n","\n","         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n","5237887                       -1         1.000317   28280361.74   0.999734   \n","5237888                       -1         1.000643    9187699.11   1.000129   \n","5237889                        0         0.995789   12725436.10   0.995789   \n","5237890                        1         0.999210   94773271.05   0.999210   \n","5237891                       -1         1.002129   24073677.32   1.000859   \n","\n","         near_price  bid_price   bid_size  ask_price   ask_size       wap  \\\n","5237887    0.999734   1.000317   32257.04   1.000434  319862.40  1.000328   \n","5237888    1.000386   1.000643  205108.40   1.000900   93393.07  1.000819   \n","5237889    0.995789   0.995789   16790.66   0.995883  180038.32  0.995797   \n","5237890    0.999210   0.998970  125631.72   0.999210  669893.00  0.999008   \n","5237891    1.001494   1.002129  250081.44   1.002447  300167.56  1.002274   \n","\n","           target  time_id       row_id  \n","5237887  2.310276    26454  480_540_195  \n","5237888 -8.220077    26454  480_540_196  \n","5237889  1.169443    26454  480_540_197  \n","5237890 -1.540184    26454  480_540_198  \n","5237891 -6.530285    26454  480_540_199  "],"text/html":["\n","  <div id=\"df-c15bf3bd-4224-48c0-a774-e54d1c95ad7a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>stock_id</th>\n","      <th>date_id</th>\n","      <th>seconds_in_bucket</th>\n","      <th>imbalance_size</th>\n","      <th>imbalance_buy_sell_flag</th>\n","      <th>reference_price</th>\n","      <th>matched_size</th>\n","      <th>far_price</th>\n","      <th>near_price</th>\n","      <th>bid_price</th>\n","      <th>bid_size</th>\n","      <th>ask_price</th>\n","      <th>ask_size</th>\n","      <th>wap</th>\n","      <th>target</th>\n","      <th>time_id</th>\n","      <th>row_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5237887</th>\n","      <td>195</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>2440722.89</td>\n","      <td>-1</td>\n","      <td>1.000317</td>\n","      <td>28280361.74</td>\n","      <td>0.999734</td>\n","      <td>0.999734</td>\n","      <td>1.000317</td>\n","      <td>32257.04</td>\n","      <td>1.000434</td>\n","      <td>319862.40</td>\n","      <td>1.000328</td>\n","      <td>2.310276</td>\n","      <td>26454</td>\n","      <td>480_540_195</td>\n","    </tr>\n","    <tr>\n","      <th>5237888</th>\n","      <td>196</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>349510.47</td>\n","      <td>-1</td>\n","      <td>1.000643</td>\n","      <td>9187699.11</td>\n","      <td>1.000129</td>\n","      <td>1.000386</td>\n","      <td>1.000643</td>\n","      <td>205108.40</td>\n","      <td>1.000900</td>\n","      <td>93393.07</td>\n","      <td>1.000819</td>\n","      <td>-8.220077</td>\n","      <td>26454</td>\n","      <td>480_540_196</td>\n","    </tr>\n","    <tr>\n","      <th>5237889</th>\n","      <td>197</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>0.00</td>\n","      <td>0</td>\n","      <td>0.995789</td>\n","      <td>12725436.10</td>\n","      <td>0.995789</td>\n","      <td>0.995789</td>\n","      <td>0.995789</td>\n","      <td>16790.66</td>\n","      <td>0.995883</td>\n","      <td>180038.32</td>\n","      <td>0.995797</td>\n","      <td>1.169443</td>\n","      <td>26454</td>\n","      <td>480_540_197</td>\n","    </tr>\n","    <tr>\n","      <th>5237890</th>\n","      <td>198</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>1000898.84</td>\n","      <td>1</td>\n","      <td>0.999210</td>\n","      <td>94773271.05</td>\n","      <td>0.999210</td>\n","      <td>0.999210</td>\n","      <td>0.998970</td>\n","      <td>125631.72</td>\n","      <td>0.999210</td>\n","      <td>669893.00</td>\n","      <td>0.999008</td>\n","      <td>-1.540184</td>\n","      <td>26454</td>\n","      <td>480_540_198</td>\n","    </tr>\n","    <tr>\n","      <th>5237891</th>\n","      <td>199</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>1884285.71</td>\n","      <td>-1</td>\n","      <td>1.002129</td>\n","      <td>24073677.32</td>\n","      <td>1.000859</td>\n","      <td>1.001494</td>\n","      <td>1.002129</td>\n","      <td>250081.44</td>\n","      <td>1.002447</td>\n","      <td>300167.56</td>\n","      <td>1.002274</td>\n","      <td>-6.530285</td>\n","      <td>26454</td>\n","      <td>480_540_199</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c15bf3bd-4224-48c0-a774-e54d1c95ad7a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c15bf3bd-4224-48c0-a774-e54d1c95ad7a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c15bf3bd-4224-48c0-a774-e54d1c95ad7a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6eef1f27-9991-4bf9-9096-e7ffe0c9d76b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6eef1f27-9991-4bf9-9096-e7ffe0c9d76b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6eef1f27-9991-4bf9-9096-e7ffe0c9d76b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":57}],"source":["df.tail()"],"id":"CZUR4yj2c71w"},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701534978596,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"8jeXH7QGc71w","outputId":"63bdc55f-e8da-4ace-a62c-8d05e6b940da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique Stock IDs:\n","[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n","  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n","  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n","  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  70  71  72\n","  73  74  75  76  77  80  81  82  83  84  85  86  87  88  89  90  91  92\n","  93  94  95  96  97  98  99 100 101 103 104 105 106 107 108 109 110 111\n"," 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129\n"," 130 131 132 133 134 136 137 138 139 140 141 142 143 144 145 146 147 148\n"," 149 151 152 154 155 157 158 159 160 161 162 163 164 165 166 167 168 169\n"," 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187\n"," 188 189 190 191 192 193 194 195 196 197 198  78  69 156 150 153 199  79\n"," 135 102]\n","\n","Unique Date IDs:\n","[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n","  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n","  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n","  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n","  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n","  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n"," 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n"," 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n"," 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n"," 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n"," 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n"," 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n"," 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n"," 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n"," 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n"," 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n"," 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n"," 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n"," 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n"," 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n"," 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n"," 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n"," 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n"," 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n"," 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n"," 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n"," 468 469 470 471 472 473 474 475 476 477 478 479 480]\n"]}],"source":["# Check unique values in stock_id\n","unique_stock_ids = df['stock_id'].unique()\n","print(\"Unique Stock IDs:\")\n","print(unique_stock_ids)\n","\n","# Check unique values in date_id\n","unique_date_ids = df['date_id'].unique()\n","print(\"\\nUnique Date IDs:\")\n","print(unique_date_ids)"],"id":"8jeXH7QGc71w"},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1701534979642,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"X8WlrKEkc71w","outputId":"4a1af924-5324-4b43-de4a-a96d851b9a04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values in each column:\n","stock_id                         0\n","date_id                          0\n","seconds_in_bucket                0\n","imbalance_size                 132\n","imbalance_buy_sell_flag          0\n","reference_price                132\n","matched_size                   132\n","far_price                  2894254\n","near_price                 2857092\n","bid_price                      132\n","bid_size                         0\n","ask_price                      132\n","ask_size                         0\n","wap                            132\n","target                           0\n","time_id                          0\n","row_id                           0\n","dtype: int64\n","\n","Are there any missing values in the DataFrame?  True\n","\n","Percentage of missing values in each column:\n","stock_id                    0.000000\n","date_id                     0.000000\n","seconds_in_bucket           0.000000\n","imbalance_size              0.002520\n","imbalance_buy_sell_flag     0.000000\n","reference_price             0.002520\n","matched_size                0.002520\n","far_price                  55.256084\n","near_price                 54.546600\n","bid_price                   0.002520\n","bid_size                    0.000000\n","ask_price                   0.002520\n","ask_size                    0.000000\n","wap                         0.002520\n","target                      0.000000\n","time_id                     0.000000\n","row_id                      0.000000\n","dtype: float64\n"]}],"source":["missing_values = df.isnull().sum()\n","print(\"Missing values in each column:\")\n","print(missing_values)\n","\n","# Check for any missing values in the entire DataFrame\n","any_missing = df.isnull().any().any()\n","print(\"\\nAre there any missing values in the DataFrame? \", any_missing)\n","\n","# Optional: Percentage of missing values in each column\n","missing_percentage = (df.isnull().sum() / len(df)) * 100\n","print(\"\\nPercentage of missing values in each column:\")\n","print(missing_percentage)"],"id":"X8WlrKEkc71w"},{"cell_type":"markdown","metadata":{"id":"45242e7d"},"source":["## üöÄ Memory Optimization Function with Data Type Conversion üßπ"],"id":"45242e7d"},{"cell_type":"markdown","metadata":{"id":"f52cd8cb"},"source":["**Explaination**\n","\n","This code defines a function `reduce_mem_usage` that is used to reduce the memory usage of a Pandas DataFrame by optimizing the data types of its columns.\n","\n","1. `def reduce_mem_usage(df, verbose=0):`\n","   - This line defines a function called `reduce_mem_usage` that takes two parameters: `df`, which is the input Pandas DataFrame that needs memory optimization, and `verbose` (defaulting to 0), which is a flag to control whether or not to provide memory optimization information.\n","\n","2. `start_mem = df.memory_usage().sum() / 1024**2`\n","   - This line calculates the initial memory usage of the input DataFrame `df` and stores it in the `start_mem` variable. It does this by using the `memory_usage()` method, which returns the memory usage of each column, and then sums these values. The result is divided by 1024^2 to convert it to megabytes.\n","\n","3. The code then enters a loop that iterates through each column of the DataFrame using the `for col in df.columns:` loop.\n","\n","4. Inside the loop, it checks the data type of the column using `col_type = df[col].dtype`.\n","\n","5. If the column's data type is not 'object' (i.e., it's numeric), it proceeds with the optimization.\n","\n","6. For integer columns:\n","   - It checks the minimum and maximum values in the column (c_min and c_max).\n","   - Depending on the range of values, it converts the column to the smallest integer data type that can accommodate the data while reducing memory usage. It checks for `int8`, `int16`, `int32`, and `int64` data types based on the data range.\n","\n","7. For float columns:\n","   - Similar to integer columns, it checks the minimum and maximum values.\n","   - It converts the column to a `float32` data type if the range is within the limits of `np.finfo(np.float32)`. The `np.finfo()` function is used to get the floating-point type's limits.\n","\n","8. If the column's data type is neither integer nor float and falls outside the specified ranges, it defaults to `float32`.\n","\n","9. If `verbose` is set to a truthy value (e.g., 1), it provides information about memory optimization, including the initial and final memory usage, and the percentage reduction in memory usage.\n","\n","10. Finally, the function returns the DataFrame with optimized memory usage.\n","\n","This function is useful for reducing the memory footprint of a DataFrame, especially when working with large datasets, by converting columns to the most memory-efficient data types based on the data they contain. It can help improve performance and reduce memory-related issues."],"id":"f52cd8cb"},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1701534979642,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"bdb0501f"},"outputs":[],"source":["# üßπ Function to reduce memory usage of a Pandas DataFrame\n","def reduce_mem_usage(df, verbose=0):\n","    \"\"\"\n","    Iterate through all numeric columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","\n","    # üìè Calculate the initial memory usage of the DataFrame\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    # üîÑ Iterate through each column in the DataFrame\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        # Check if the column's data type is not 'object' (i.e., numeric)\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","\n","            # Check if the column's data type is an integer\n","            if str(col_type)[:3] == \"int\":\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                # Check if the column's data type is a float\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float32)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float32)\n","\n","    # ‚ÑπÔ∏è Provide memory optimization information if 'verbose' is True\n","    if verbose:\n","        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n","        end_mem = df.memory_usage().sum() / 1024**2\n","        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n","        decrease = 100 * (start_mem - end_mem) / start_mem\n","        logger.info(f\"Decreased by {decrease:.2f}%\")\n","\n","    # üîÑ Return the DataFrame with optimized memory usage\n","    return df\n"],"id":"bdb0501f"},{"cell_type":"markdown","metadata":{"id":"c7c042cb"},"source":[" ## üèéÔ∏èParallel Triplet Imbalance Calculation with Numba"],"id":"c7c042cb"},{"cell_type":"markdown","metadata":{"id":"c4515380"},"source":["**Explaination**\n","\n","\n","This code includes functions for calculating triplet imbalance in a parallel and optimized manner using the Numba library. Let's break down each part of the code:\n","\n","1. `from numba import njit, prange`\n","   - This line imports two important features from the Numba library: `njit` for Just-In-Time (JIT) compilation and `prange` for parallel processing. JIT compilation can significantly speed up the execution of code, and parallel processing allows for concurrent execution of code in a loop.\n","\n","2. `@njit(parallel=True)`\n","   - This is a decorator applied to the `compute_triplet_imbalance` function, indicating that Numba should compile this function for speed optimization and parallel execution. This decorator makes use of Numba's features to enhance the performance of the code.\n","\n","3. `def compute_triplet_imbalance(df_values, comb_indices):`\n","   - This function is designed to calculate triplet imbalance in a parallelized manner using Numba. It takes two parameters:\n","     - `df_values`: A NumPy array containing the values of the DataFrame. It represents the price data.\n","     - `comb_indices`: A list of combinations of three price indices (a, b, c) for which triplet imbalance needs to be computed.\n","\n","4. `num_rows = df_values.shape[0]`\n","   - This line calculates the number of rows in the `df_values` array, which represents the number of rows in the DataFrame.\n","\n","5. `imbalance_features = np.empty((num_rows, num_combinations))`\n","   - This line initializes an empty NumPy array `imbalance_features` with dimensions (number of rows, number of combinations). This array will store the computed triplet imbalance values.\n","\n","6. The code then enters a loop that iterates through all combinations of triplets specified by `comb_indices`.\n","\n","7. `for i in prange(num_combinations):`\n","   - This loop is parallelized using `prange`, which allows for multiple combinations to be processed concurrently.\n","\n","8. Inside the loop, it extracts the indices (a, b, c) for the current combination.\n","\n","9. Another loop iterates through the rows of the DataFrame (`for j in range(num_rows)`) and calculates the triplet imbalance for each row.\n","\n","10. `max_val`, `min_val`, and `mid_val` are computed for each row.\n","\n","11. `if mid_val == min_val:` checks if division by zero would occur and sets the corresponding entry in `imbalance_features` to `np.nan` to prevent errors in such cases.\n","\n","12. The final imbalance value is calculated using the formula `(max_val - mid_val) / (mid_val - min_val)` and stored in the `imbalance_features` array.\n","\n","13. The function returns the `imbalance_features` array, which contains the computed triplet imbalance values for all combinations and rows.\n","\n","14. `calculate_triplet_imbalance_numba` is another function that takes a price column name and a DataFrame as input. It prepares the data and calculates triplet imbalance using the `compute_triplet_imbalance` function. It returns the result as a DataFrame with appropriately labeled columns.\n"],"id":"c4515380"},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1701534979642,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"ed0fa761"},"outputs":[],"source":["# üèéÔ∏è Import Numba for just-in-time (JIT) compilation and parallel processing\n","from numba import njit, prange\n","\n","# üìä Function to compute triplet imbalance in parallel using Numba\n","@njit(parallel=True)\n","def compute_triplet_imbalance(df_values, comb_indices):\n","    num_rows = df_values.shape[0]\n","    num_combinations = len(comb_indices)\n","    imbalance_features = np.empty((num_rows, num_combinations))\n","\n","    # üîÅ Loop through all combinations of triplets\n","    for i in prange(num_combinations):\n","        a, b, c = comb_indices[i]\n","\n","        # üîÅ Loop through rows of the DataFrame\n","        for j in range(num_rows):\n","            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n","            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n","            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n","\n","            # üö´ Prevent division by zero\n","            if mid_val == min_val:\n","                imbalance_features[j, i] = np.nan\n","            else:\n","                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n","\n","    return imbalance_features\n","\n","# üìà Function to calculate triplet imbalance for given price data and a DataFrame\n","def calculate_triplet_imbalance_numba(price, df):\n","    # Convert DataFrame to numpy array for Numba compatibility\n","    df_values = df[price].values\n","    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n","\n","    # Calculate the triplet imbalance using the Numba-optimized function\n","    features_array = compute_triplet_imbalance(df_values, comb_indices)\n","\n","    # Create a DataFrame from the results\n","    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n","    features = pd.DataFrame(features_array, columns=columns)\n","\n","    return features\n"],"id":"ed0fa761"},{"cell_type":"markdown","metadata":{"id":"1b190d6e"},"source":["## üìä Feature Generation Functions üìä\n","\n","\n","\n","\n"],"id":"1b190d6e"},{"cell_type":"markdown","metadata":{"id":"5d6e3fcc"},"source":["**Explaination**\n","\n","\n","\n","1. `imbalance_features(df)`:\n","   - This function takes a DataFrame `df` as input.\n","   - It calculates various features related to price and size data using Pandas' `eval` function, creating new columns in the DataFrame for each feature.\n","   - It then creates pairwise price imbalance features for combinations of price columns.\n","   - Next, it calculates triplet imbalance features using the Numba-optimized function `calculate_triplet_imbalance_numba`.\n","   - Finally, it calculates additional features, including momentum, spread, intensity, pressure, market urgency, and depth pressure.\n","   - It also calculates statistical aggregation features (mean, standard deviation, skewness, kurtosis) for both price and size columns.\n","   - Shifted, return, and diff features are generated for specific columns.\n","   - Infinite values in the DataFrame are replaced with 0.\n","\n","2. `other_features(df)`:\n","   - This function adds time-related and stock-related features to the DataFrame.\n","   - It calculates the day of the week, seconds, and minutes from the \"date_id\" and \"seconds_in_bucket\" columns.\n","   - It maps global features from a predefined dictionary to the DataFrame based on the \"stock_id.\"\n","\n","3. `generate_all_features(df)`:\n","   - This function combines the features generated by the `imbalance_features` and `other_features` functions.\n","   - It selects the relevant columns for feature generation, applies the `imbalance_features` function, adds time and stock-related features using the `other_features` function, and then performs garbage collection to free up memory.\n","   - The function returns a DataFrame containing the generated features, excluding certain columns like \"row_id,\" \"target,\" \"time_id,\" and \"date_id.\"\n"],"id":"5d6e3fcc"},{"cell_type":"code","source":["pip in"],"metadata":{"id":"fseyjg6wlgI_"},"id":"fseyjg6wlgI_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1701534979642,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"33b29ee2"},"outputs":[],"source":["# üìä Function to generate imbalance features\n","def imbalance_features(df):\n","    # Define lists of price and size-related column names\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","\n","    # V1 features\n","    # Calculate various features using Pandas eval function\n","    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n","    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n","    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n","    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n","    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n","\n","    # Create features for pairwise price imbalances\n","    for c in combinations(prices, 2):\n","        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n","\n","    # Calculate triplet imbalance features using the Numba-optimized function\n","    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n","        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n","        df[triplet_feature.columns] = triplet_feature.values\n","\n","    # V2 features\n","    # Calculate additional features\n","    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n","    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n","    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n","    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n","    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n","    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n","\n","    # Calculate various statistical aggregation features\n","    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n","        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n","        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n","\n","    # V3 features\n","    # Calculate shifted and return features for specific columns\n","    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n","            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n","\n","    # Calculate diff features for specific columns\n","    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n","\n","    # Replace infinite values with 0\n","    return df.replace([np.inf, -np.inf], 0)\n","\n","# üìÖ Function to generate time and stock-related features\n","def other_features(df):\n","    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n","    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n","    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n","\n","    # Map global features to the DataFrame\n","    for key, value in global_stock_id_feats.items():\n","        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n","\n","    return df\n","\n","# üöÄ Function to generate all features by combining imbalance and other features\n","def generate_all_features(df):\n","    # Select relevant columns for feature generation\n","    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n","    df = df[cols]\n","\n","    # Generate imbalance features\n","    df = imbalance_features(df)\n","\n","    # Generate time and stock-related features\n","    df = other_features(df)\n","    gc.collect()  # Perform garbage collection to free up memory\n","\n","    # Select and return the generated features\n","    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n","\n","    return df[feature_name]\n"],"id":"33b29ee2"},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1701534979642,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"d733e8fe"},"outputs":[],"source":["weights = [\n","    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n","    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n","    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n","    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n","    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n","    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n","    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n","    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n","    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n","    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n","    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n","    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n","    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n","    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n","    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n","    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n","    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n","]\n","\n","weights = {int(k):v for k,v in enumerate(weights)}"],"id":"d733e8fe"},{"cell_type":"markdown","metadata":{"id":"b4ec1d78"},"source":["## Data Splitting"],"id":"b4ec1d78"},{"cell_type":"markdown","metadata":{"id":"0a4aced7"},"source":["**Explaination**\n","\n","Checks whether it is running in offline or online mode and takes different actions accordingly. Here's what each part of the code does:\n","\n","1. `if is_offline:`:\n","   - This condition checks if the variable `is_offline` is `True`. If it is `True`, it means the code is running in offline mode.\n","\n","2. In the offline mode block:\n","   - The code splits the dataset into two parts: `df_train` and `df_valid` based on the value of the `split_day`. Data with \"date_id\" less than or equal to the `split_day` is assigned to `df_train`, while data with \"date_id\" greater than the `split_day` is assigned to `df_valid`.\n","   - It then displays a message indicating that the code is running in offline mode and provides the shapes (number of rows and columns) of the training and validation sets using the `print` statements.\n","\n","3. In the online mode block:\n","   - If the code is not running in offline mode (i.e., `is_offline` is `False`), it means it's running in online mode.\n","   - In online mode, the entire dataset is used for training, and the entire dataset is assigned to `df_train`.\n","   - It displays a message indicating that the code is running in online mode using the `print` statement.\n","\n","The purpose of distinguishing between offline and online modes is often related to the context in which the code is used. In offline mode, you typically have historical data and can perform tasks like data splitting for training and validation, while in online mode, you might be working with real-time data and use the entire dataset for training. The choice of mode can impact the preprocessing and analysis steps that follow in the code."],"id":"0a4aced7"},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1701534979643,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"},"user_tz":300},"id":"ae169c83","outputId":"16b81085-dd6e-4bef-d8ac-d945ed6dd1d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Online mode\n"]}],"source":["# Check if the code is running in offline or online mode\n","if is_offline:\n","    # In offline mode, split the data into training and validation sets based on the split_day\n","    df_train = df[df[\"date_id\"] <= split_day]\n","    df_valid = df[df[\"date_id\"] > split_day]\n","\n","    # Display a message indicating offline mode and the shapes of the training and validation sets\n","    print(\"Offline mode\")\n","    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n","else:\n","    # In online mode, use the entire dataset for training\n","    df_train = df\n","\n","    # Display a message indicating online mode\n","    print(\"Online mode\")\n"],"id":"ae169c83"},{"cell_type":"markdown","metadata":{"id":"5b397836"},"source":["**Explaination**\n","\n","\n","\n","1. `if is_train:`\n","   - This condition checks if the variable `is_train` is `True`. If it is `True`, it means that the code is being executed in a training context.\n","\n","2. Inside the `if is_train:` block:\n","   - A dictionary named `global_stock_id_feats` is created. This dictionary contains various statistical summary features calculated for each stock_id. These features include the median, standard deviation, and range of bid sizes and ask sizes, as well as bid prices and ask prices. These statistics are computed based on the training data (`df_train`) using Pandas' `groupby` and aggregation functions.\n","\n","3. The code checks if the execution mode is offline (`is_offline`) by further nested conditions.\n","   - If it is offline (`is_offline` is `True`):\n","     - It generates features for the training set (`df_train`) using the `generate_all_features` function.\n","     - It prints a message indicating that the process of building the training features is finished.\n","     - It generates features for the validation set (`df_valid`) using the `generate_all_features` function.\n","     - It prints a message indicating that the process of building the validation features is finished.\n","     - It reduces memory usage of the validation features using the `reduce_mem_usage` function.\n","\n","   - If it is not in offline mode (i.e., online mode):\n","     - It generates features for the training set (`df_train`) using the `generate_all_features` function.\n","     - It prints a message indicating that the process of building online training features is finished.\n","\n","4. After generating features, it reduces memory usage of the training features (`df_train_feats`) using the `reduce_mem_usage` function. This is done to optimize memory consumption and improve performance.\n","\n","The code's purpose is to prepare and optimize the feature set for training, considering whether it is in offline or online mode and whether it's part of the training process. The generated features and memory optimization are important steps in machine learning workflows, as they impact the training process and the model's efficiency."],"id":"5b397836"},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70342286","executionInfo":{"status":"ok","timestamp":1701535032069,"user_tz":300,"elapsed":52431,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"}},"outputId":"fa7375d4-b114-496e-b71f-011ce70f86c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Build Online Train Feats Finished.\n"]}],"source":["if is_train:\n","    global_stock_id_feats = {\n","        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n","        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n","        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n","        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n","        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n","        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n","    }\n","    if is_offline:\n","        df_train_feats = generate_all_features(df_train)\n","        print(\"Build Train Feats Finished.\")\n","        df_valid_feats = generate_all_features(df_valid)\n","        print(\"Build Valid Feats Finished.\")\n","        df_valid_feats = reduce_mem_usage(df_valid_feats)\n","    else:\n","        df_train_feats = generate_all_features(df_train)\n","        print(\"Build Online Train Feats Finished.\")\n","\n","    df_train_feats = reduce_mem_usage(df_train_feats)\n"],"id":"70342286"},{"cell_type":"markdown","metadata":{"id":"8b4f8996"},"source":["**Exaplaination**\n","\n","This code block is responsible for training a machine learning model (LightGBM) in a context where `is_train` is `True`. It also performs inference and evaluates the model if it's in offline mode.\n","\n","1. `if is_train:`\n","   - This condition checks if the variable `is_train` is `True`. If it is `True`, it means that the code is being executed in a training context.\n","\n","2. Inside the `if is_train:` block:\n","   - It gets the list of feature names from the training features (`df_train_feats`).\n","\n","3. LightGBM Parameters:\n","   - It defines LightGBM parameters for the model, specifying various hyperparameters such as the objective function, number of estimators, number of leaves, subsample ratio, learning rate, number of CPU cores to use, GPU acceleration, and others.\n","   - It prints the length of the feature names to check the number of features used in the model.\n","\n","4. Offline Split:\n","   - It creates a mask (`offline_split`) to split the training data into two sets based on a specific date (in this case, `(split_day - 45)`). Data with a \"date_id\" greater than this date is considered for offline validation, and data with a \"date_id\" less than or equal to this date is considered for offline training.\n","   - It creates separate DataFrames for offline training (`df_offline_train` and `df_offline_train_target`) and offline validation (`df_offline_valid` and `df_offline_valid_target`).\n","   - It prints a message indicating that offline model training is taking place.\n","\n","5. Train LightGBM Model:\n","   - It creates a LightGBM Regressor model (`lgb_model`) with the specified parameters and fits it to the offline training data.\n","   - It sets up early stopping and evaluation callbacks.\n","   - This model is trained on the offline data.\n","\n","6. Memory Cleanup:\n","   - It frees up memory by deleting variables related to offline training and performing garbage collection.\n","\n","7. Inference:\n","   - It defines the target variable for the entire training dataset (`df_train_target`) and prints a message indicating that inference model training is taking place.\n","   - It creates an inference model (`infer_lgb_model`) that is a copy of the initial model but with the number of estimators adjusted based on the best iteration from early stopping.\n","\n","8. If in offline mode (`is_offline` is `True`):\n","   - It performs offline predictions using the inference model on the validation set (`df_valid_feats`) and evaluates the predictions using the mean absolute error (`mean_absolute_error`) against the true target values (`df_valid_target`).\n","   - It prints the offline score as a measure of model performance on the validation set.\n","\n","This code is designed to train and evaluate a machine learning model for offline data, and the training strategy includes early stopping and adjusting the number of estimators during inference to optimize model performance. It also includes memory management steps to improve the efficiency of the training process."],"id":"8b4f8996"},{"cell_type":"code","source":["!pip install numpy==1.25.0\n","!pip install typing-extensions==4.5.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"sBb-hoP9RAoL","executionInfo":{"status":"ok","timestamp":1701535044468,"user_tz":300,"elapsed":12417,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"}},"outputId":"7bf806a1-b679-46ee-a5e7-0858aecb0273"},"id":"sBb-hoP9RAoL","execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.25.0\n","  Using cached numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.2\n","    Uninstalling numpy-1.26.2:\n","      Successfully uninstalled numpy-1.26.2\n","Successfully installed numpy-1.25.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (4.5.0)\n"]}]},{"cell_type":"code","source":["!pip install \\\n","    --force-reinstall \\\n","    --no-binary lightgbm \\\n","    --config-settings=cmake.define.USE_CUDA=ON \\\n","    --config-settings=cmake.define.DUSE_CUDAP=1 \\\n","    lightgbm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u75Ynu7jFtaP","executionInfo":{"status":"ok","timestamp":1701535057263,"user_tz":300,"elapsed":12799,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"}},"outputId":"29b4d679-2e5e-494c-81f5-738e5eb8885c"},"id":"u75Ynu7jFtaP","execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightgbm\n","  Using cached lightgbm-4.1.0-py3-none-manylinux_2_35_x86_64.whl\n","Collecting numpy (from lightgbm)\n","  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Collecting scipy (from lightgbm)\n","  Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n","Installing collected packages: numpy, scipy, lightgbm\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.0\n","    Uninstalling numpy-1.25.0:\n","      Successfully uninstalled numpy-1.25.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.11.4\n","    Uninstalling scipy-1.11.4:\n","      Successfully uninstalled scipy-1.11.4\n","  Attempting uninstall: lightgbm\n","    Found existing installation: lightgbm 4.1.0\n","    Uninstalling lightgbm-4.1.0:\n","      Successfully uninstalled lightgbm-4.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed lightgbm-4.1.0 numpy-1.26.2 scipy-1.11.4\n"]}]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"id":"fb88021e","executionInfo":{"status":"error","timestamp":1701535057263,"user_tz":300,"elapsed":21,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"}},"outputId":"0417b042-48b5-4414-9f83-061314204701"},"outputs":[{"output_type":"stream","name":"stdout","text":["Feature length = 112\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-cca8c22a3f72>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Exclude the purged ranges from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mpurged_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_ids\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mpurged_before_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdate_ids\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpurged_before_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                  \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_ids\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mpurged_after_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdate_ids\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpurged_after_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5237892,) (481,) "]}],"source":["import numpy as np\n","import lightgbm as lgb\n","from sklearn.metrics import mean_absolute_error\n","import gc\n","\n","# Assuming df_train_feats and df_train are already defined and df_train contains the 'date_id' column\n","\n","# Set up parameters for LightGBM\n","\n","lgb_params = {\n","        \"objective\": \"mae\",\n","        \"n_estimators\": 5500,\n","        \"num_leaves\": 256,\n","        \"subsample\": 0.6,\n","        \"colsample_bytree\": 0.6,\n","        \"learning_rate\": 0.00877,\n","        \"n_jobs\": 4,\n","        \"device\": \"cuda\",\n","        \"verbosity\": -1,\n","        \"importance_type\": \"gain\",\n","        \"max_depth\": 12,  # Maximum depth of the tree\n","        \"min_child_samples\": 15,  # Minimum number of data points in a leaf\n","        \"reg_alpha\": 0.1,  # L1 regularization term\n","        \"reg_lambda\": 0.3,  # L2 regularization term\n","        \"min_split_gain\": 0.2,  # Minimum loss reduction required for further partitioning\n","        \"min_child_weight\": 0.001,  # Minimum sum of instance weight (hessian) in a leaf\n","        \"bagging_fraction\": 0.9,  # Fraction of data to be used for training each tree\n","        \"bagging_freq\": 5,  # Frequency for bagging\n","        \"feature_fraction\": 0.9,  # Fraction of features to be used for training each tree\n","        \"num_threads\": 4,  # Number of threads for LightGBM to use\n","}\n","feature_name = list(df_train_feats.columns)\n","print(f\"Feature length = {len(feature_name)}\")\n","\n","# The total number of date_ids is 480, we split them into 5 folds with a gap of 5 days in between\n","num_folds = 5\n","fold_size = unique_date_ids // num_folds\n","gap = 5\n","\n","models = []\n","scores = []\n","\n","model_save_path = 'modelitos_para_despues'  # Directory to save models\n","if not os.path.exists(model_save_path):\n","    os.makedirs(model_save_path)\n","\n","# We need to use the date_id from df_train to split the data\n","date_ids = df_train['date_id'].values\n","\n","for i in range(num_folds):\n","    start = i * fold_size\n","    end = start + fold_size\n","\n","    # Define the purged set ranges\n","    purged_before_start = start - 2\n","    purged_before_end = start + 2\n","    purged_after_start = end - 2\n","    purged_after_end = end + 2\n","\n","    # Exclude the purged ranges from the test set\n","    purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n","                 ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n","\n","    # Define test_indices excluding the purged set\n","    test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n","    train_indices = ~test_indices & ~purged_set\n","\n","    df_fold_train = df_train_feats[train_indices]\n","    df_fold_train_target = df_train['target'][train_indices]\n","    df_fold_valid = df_train_feats[test_indices]\n","    df_fold_valid_target = df_train['target'][test_indices]\n","\n","    print(f\"Fold {i+1} Model Training\")\n","\n","    # Train a LightGBM model for the current fold\n","    lgb_model = lgb.LGBMRegressor(**lgb_params)\n","    lgb_model.fit(\n","        df_fold_train[feature_name],\n","        df_fold_train_target,\n","        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n","        callbacks=[\n","            lgb.callback.early_stopping(stopping_rounds=100),\n","            lgb.callback.log_evaluation(period=100),\n","        ],\n","    )\n","\n","    # Append the model to the list\n","    models.append(lgb_model)\n","    # Save the model to a file\n","    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n","    lgb_model.booster_.save_model(model_filename)\n","    print(f\"Model for fold {i+1} saved to {model_filename}\")\n","\n","    # Evaluate model performance on the validation set\n","    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n","    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n","    scores.append(fold_score)\n","    print(f\"Fold {i+1} MAE: {fold_score}\")\n","\n","    # Free up memory by deleting fold specific variables\n","    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n","    gc.collect()\n","\n","# Calculate the average best iteration from all regular folds\n","average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n","\n","# Update the lgb_params with the average best iteration\n","final_model_params = lgb_params.copy()\n","final_model_params['n_estimators'] = average_best_iteration\n","\n","print(f\"Training final model with average best iteration: {average_best_iteration}\")\n","\n","# Train the final model on the entire dataset\n","final_model = lgb.LGBMRegressor(**final_model_params)\n","final_model.fit(\n","    df_train_feats[feature_name],\n","    df_train['target'],\n","    callbacks=[\n","        lgb.callback.log_evaluation(period=100),\n","    ],\n",")\n","\n","# Append the final model to the list of models\n","models.append(final_model)\n","\n","# Save the final model to a file\n","final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n","final_model.booster_.save_model(final_model_filename)\n","print(f\"Final model saved to {final_model_filename}\")\n","\n","# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n","print(f\"Average MAE across all folds: {np.mean(scores)}\")"],"id":"fb88021e"},{"cell_type":"markdown","metadata":{"id":"a39e6604"},"source":["**Explaination**\n","\n","This code block is responsible for making predictions in inference mode and submitting them to the Optiver 2023 competition environment.\n","\n","1. `def zero_sum(prices, volumes):`\n","   - This function takes two NumPy arrays, `prices` and `volumes`, as input.\n","   - It calculates the standard error as the square root of the `volumes`.\n","   - It calculates a variable `step` by dividing the sum of `prices` by the sum of the standard errors (`std_error`).\n","   - It calculates the `out` variable as the difference between the `prices` and the product of the `std_error` and the `step`.\n","   - The function returns the `out` variable.\n","\n","2. `if is_infer:`\n","   - This condition checks if the variable `is_infer` is `True`. If it is `True`, it means that the code is being executed in inference mode.\n","\n","3. Inside the `if is_infer:` block:\n","   - The code imports the `optiver2023` module and creates an environment (`env`) for the Optiver 2023 competition.\n","\n","4. It initializes variables, including an iterator (`iter_test`), a counter (`counter`), and variables for specifying the lower and upper limits of predictions (`y_min` and `y_max`).\n","\n","5. It also initializes lists for recording queries per second (`qps`) and a DataFrame (`cache`) to store test data.\n","\n","6. The code enters a loop that iterates through the test data provided by the environment.\n","\n","7. Inside the loop:\n","   - It records the current time (`now_time`) using the `time.time()` function.\n","   - It concatenates the current test data with the existing cache of data.\n","   - It keeps only the most recent 21 rows for each stock and sorts them.\n","   - It generates features for the current test data using the `generate_all_features` function.\n","\n","8. It makes predictions using the previously trained inference LightGBM model (`infer_lgb_model`).\n","\n","9. It applies the `zero_sum` function to transform the predictions.\n","\n","10. It clips the transformed predictions to ensure they fall within the specified range defined by `y_min` and `y_max`.\n","\n","11. It updates the sample prediction with the clipped values.\n","\n","12. It submits the predictions to the environment using the `env.predict()` method.\n","\n","13. It updates the counter and records the time spent on each iteration in the `qps` list.\n","\n","14. It prints the current iteration number and the average queries per second (qps) if the counter is a multiple of 10.\n","\n","15. After processing all test data, it calculates the estimated time cost based on the average qps and prints the estimated time to reason about.\n","\n","This code is designed for making predictions in an Optiver trading competition environment and uses a trained LightGBM model for inference. It also includes a transformation step (`zero_sum`) and clipping of predictions to ensure they are within a specified range before submitting them to the competition environment."],"id":"a39e6604"},{"cell_type":"code","execution_count":null,"metadata":{"id":"38cc7c36","executionInfo":{"status":"aborted","timestamp":1701535057264,"user_tz":300,"elapsed":6,"user":{"displayName":"Zhan Wei Goh","userId":"06506265241458279276"}}},"outputs":[],"source":["def zero_sum(prices, volumes):\n","    std_error = np.sqrt(volumes)\n","    step = np.sum(prices) / np.sum(std_error)\n","    out = prices - std_error * step\n","    return out\n","\n","if is_infer:\n","    import optiver2023\n","    env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","    counter = 0\n","    y_min, y_max = -64, 64\n","    qps, predictions = [], []\n","    cache = pd.DataFrame()\n","\n","    # Weights for each fold model\n","    model_weights = [1/len(models)] * len(models)\n","\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        now_time = time.time()\n","        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n","        if counter > 0:\n","            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","        feat = generate_all_features(cache)[-len(test):]\n","\n","        # Generate predictions for each model and calculate the weighted average\n","        lgb_predictions = np.zeros(len(test))\n","        for model, weight in zip(models, model_weights):\n","            lgb_predictions += weight * model.predict(feat)\n","\n","        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n","        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n","        sample_prediction['target'] = clipped_predictions\n","        env.predict(sample_prediction)\n","        counter += 1\n","        qps.append(time.time() - now_time)\n","        if counter % 10 == 0:\n","            print(counter, 'qps:', np.mean(qps))\n","\n","    time_cost = 1.146 * np.mean(qps)\n","    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"],"id":"38cc7c36"},{"cell_type":"markdown","metadata":{"id":"85229e73"},"source":["## Keep Exploring! üëÄ\n","\n","If you enjoyed exploring this notebook and found it insightful, I encourage you to delve further into my portfolio.\n","\n","üëâ [Explore My Portfolio](https://www.kaggle.com/zulqarnainali) üëà\n","\n","## Share Your Feedback and Gratitude üôè\n","\n","Your feedback is invaluable to us! We welcome your insights, suggestions, and questions as they fuel our continuous growth. If you have any comments or ideas to share, please feel free to get in touch with us.\n","\n","üì¨ Contact us via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n","\n","We extend our heartfelt gratitude for your time and engagement. Your support inspires us to generate more valuable content.\n","\n","Wishing you a rewarding journey in the realm of data science and coding! üöÄ"],"id":"85229e73"},{"cell_type":"markdown","metadata":{"id":"ddeiepYqc710"},"source":[],"id":"ddeiepYqc710"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"papermill":{"default_parameters":{},"duration":8004.917389,"end_time":"2023-11-06T12:22:40.186190","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-06T10:09:15.268801","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}